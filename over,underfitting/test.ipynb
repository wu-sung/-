{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 오버피팅 (과대 적합) , 즉 학습 데이터에 너무 편향되어 학습이 되어 있는 경우\n",
    "\n",
    "# 원인 \n",
    "# ====================================================================================\n",
    "# 데이터 - 학습데이터가 다양한 특징을 반영하지 못하고\n",
    "#편향되어 있는 경우, 데이터 자체가 부족한경우, 데이터의 특징이 너무 다양해서 결정 경계를 만들기 어려운 경우\n",
    "\n",
    "#학습 알고리즘\n",
    "# Loss 함수에 의해 오차 자체가 너무 데이터에 fit하게 모델이 학습하는 경우\n",
    "\n",
    "# 모델\n",
    "# 데이터에 비해 너무 복잡한 모델을 활용하는 경우  ->  hidden unit에 의한 오류\n",
    "# 모델에 너무 많은 정보를 담으려고 하는 경우 -> hidden unit\n",
    "# 모델을 필요 이상으로 학습을 진행하는 경우 -> epoch 횟수 오류\n",
    "# ===================================================================================="
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T04:12:06.857139900Z",
     "start_time": "2024-02-28T04:12:06.814148700Z"
    }
   },
   "id": "e7d0122e913955b5",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 데이터 증가 \n",
    "# 모델이 더 다양한 분포의 데이터를 학습 가능하도록 유도 해야함\n",
    "# 하지만, 단순히 데이터를 증가하는 것은 효과가 크지 않고 많은 비용이 발생하여 비효율적임\n",
    "\n",
    "# Data augmentation\n",
    "# 제한된 데이터 내에서 다양성을 부여하여 일반적 특징을 학습시키기 위한 방법\n",
    "# 데이터를 Rotate, Cut, Flip 등을 하여 변형시킴"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T04:58:03.200569800Z",
     "start_time": "2024-02-28T04:58:03.075566400Z"
    }
   },
   "id": "121f9f75e2bf0941",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Feature selection -> 다차원을 줄여 학습에 효율적으로 \n",
    "# 제한된 데이터 내에서 데이터를 일반화 시키기 위한 방법\n",
    "# 딥러닝에서는 잘 활용되지 않지만, 데이터를 더 늘릴 수 없는 경우 효과적\n",
    "\n",
    "# Cross-validation\n",
    "# 학습 데이터와 테스트 데이터를 K그룹으로 분리하여, 각 epoch마다 Test데이터를 교차 적용하는 법\n",
    "# Test 데이터를 모두 학습에 사용\n",
    "# 역시 딥러닝에서는 잘 활용되지 않지만 데이터가 적은 경우에는 활용 할 수 있음"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T05:05:26.439051600Z",
     "start_time": "2024-02-28T05:05:26.426044300Z"
    }
   },
   "id": "bb1b3429810f61c",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Regularization\n",
    "# Loss 함수 결과값에 너무 fit해서 학습하는 것을 방지\n",
    "# 즉, Error 값에 대한 패널티를 부여하는 것\n",
    "# L = Error(y, y^) + 람다regularization\n",
    "\n",
    "# Balancing the error between regularization term\n",
    "# Regularization term이 포함된 loss가 가장 저점이 되는 지점은 두 point가 맞닿는 부분\n",
    "# 맞닿는 지점을 만들기 위해서는 두 값 중 하나는 증가해야 함. 증가 없이는 교차점을 만들 수 없음\n",
    "\n",
    "# \"Minimizes data term\"과 \"Minimizes combination\" 그리고 \"Minimizes regularization\"은 기계 학습에서 최적화 과정에서 나타나는 용어입니다.\n",
    "# \n",
    "# Minimizes data term (데이터 항을 최소화): 모델이 주어진 데이터에 대해 얼마나 잘 맞는지 측정하는 항목입니다. 일반적으로 손실 함수(loss function)를 통해 모델의 예측이 실제 데이터와 얼마나 일치하는지 평가하고, 최소화하려고 노력합니다.\n",
    "# \n",
    "# Minimizes combination (조합 항을 최소화): 여러 항목이 조합된 함수에서 특정 항을 최소화하려는 의미일 수 있습니다. 이는 주로 복잡한 최적화 문제에서 여러 구성 요소 간의 상호 작용을 최소화하고자 하는 목적을 나타냅니다.\n",
    "# \n",
    "# Minimizes regularization (정규화 항을 최소화): 정규화는 모델이 훈련 데이터에 지나치게 적합되는 것을 방지하기 위해 모델의 복잡성을 제어하는 기법입니다. L1 정규화, L2 정규화 등이 있으며, 이러한 정규화 항을 최소화하면서 모델의 일반화 능력을 향상시키려고 합니다."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T06:42:18.837297700Z",
     "start_time": "2024-02-28T06:42:18.786295400Z"
    }
   },
   "id": "7a0c3d7dc49ad30b",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Norm \n",
    "# 벡터의 크기를 나타내는 값\n",
    "\n",
    "# L1 Norm\n",
    "# 흔히 Manhattan distance라고 말하고 두 벡틩 최단 거리가 아닌 벡터의 모든 성분의 절대값을 더한 값\n",
    "# ||x||1= 시그마 i=1부터 n까지 1|xj|\n",
    "\n",
    "# 2차원 평면에서 손실값을 생각해보자\n",
    "# SGD나 다른 gradiant 방식으로 평탄하게 만드는데 이 때 최저점에 맞게 normalize가 된다면 \n",
    "# 오버피팅이 된다. 이를 방지하기 위해 L1 Norm, L2 Norm을 사용하여 이를 방지한다.\n",
    "# y = x1 +x2로 마름모가 될지\n",
    "# y = x1^2 + x2^2로 원형을 유지할지\n",
    "# 무조건적으로 2차 평면이아니라 파라미터에 따라 다르다.\n",
    "# Error(y, p) + 람다*(regularization) -> 정답과 예측값 + L1, L2 Norm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T07:37:09.242350800Z",
     "start_time": "2024-02-28T07:37:09.162352800Z"
    }
   },
   "id": "1af57211df50ab99",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 모델 간소화\n",
    "\n",
    "# Remove Hidden units or Layers\n",
    "\n",
    "# 데이터에 비해 모델이 너무 복잡한 경우, 모델을 간소화 시킬 수 있음\n",
    "# 모델은 간소화 시킨다는 것은 모델의 크기(깊이 또는 넓이)를 줄이는 것을 읨\n",
    "\n",
    "# Dropout\n",
    "# Dropout은 학습하는 단계에서 몇 개의 neuron은 비활성화 상태로 만들고, Test 단계에서 모두 활성화 하는 방법\n",
    "# hidden Unit을 줄이는 효과와 비슷함.\n",
    "\n",
    "# Early stopping\n",
    "# 모델 학습에서 Validation set의 오류가 증가하는 현상에서 학습을 중지하는 방법\n",
    "# Overfitting을 예방하는 가장 쉬운 방법이나, 아직 학습이 덜 되었을 가능성에서 좋은 방법은 아님"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T07:48:26.529293400Z",
     "start_time": "2024-02-28T07:48:26.432295800Z"
    }
   },
   "id": "9f0a39809d6f7c0b",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 과소 적합\n",
    "# Underfitting \n",
    "# 모델이 학습 데이터에서 입력 데이터와 출력 결과에서 관계를 잘 못 찾아 내는 현상\n",
    "# 학습 데이터와 테스트데이터 모두에서 정확도가 낮게 측정 될때 이 현상이 발생하고 있음을 추정\n",
    "# 데이터에 비해 모델이 너무 간단하거나, 학습 시간이 짧거나, learning rate가 적합하지 안ㅇ흘 때 발생"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T07:50:16.053873900Z",
     "start_time": "2024-02-28T07:50:15.959873600Z"
    }
   },
   "id": "51098eb18bb95d51",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 해결 방안\n",
    "\n",
    "# Decrease regularization\n",
    "# 정규화는 overfitting에서 확인했듯, 모델의 복잡도를 낮추기 위해 사용\n",
    "# 이를 반대로 적용하여, regularization을 낮추는 것이 underfitting에서 도움이 될 수 있음\n",
    "\n",
    "# Increase the duration of training (이 방법을 자주 씀)\n",
    "# 학습이 아직 되고 있지 않은 시점일 수 있음\n",
    "# 따라서 epoch을 더 늘리거나, learning rate를 조절하는 것이 도움이 됨\n",
    "\n",
    "# Feature selection ( 이 방법은 추천하지 않음)\n",
    "# 모델에 비해 데이터가 너무 복잡 하면서 발생 가능 할 수 있음\n",
    "# 데이터의 양이 적고, feature가 많은 경우 불필요한 정보를 삭제하는 것이 도움 됨"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T02:33:58.202015900Z",
     "start_time": "2024-02-29T02:33:58.186019400Z"
    }
   },
   "id": "f9a7a633092d084a",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets   \n",
    "from torchvision.transforms import ToTensor,Normalize, Compose\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T03:59:11.204648400Z",
     "start_time": "2024-02-29T03:59:11.162135800Z"
    }
   },
   "id": "f9a6338e762e66c8",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = Compose(\n",
    "    [ToTensor(),\n",
    "    Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# CIFAR 32x32 3channel\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'derr', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T03:59:32.096112500Z",
     "start_time": "2024-02-29T03:59:11.676713700Z"
    }
   },
   "id": "e7f5eeb542473af6",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(32*32*3, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),       \n",
    "            nn.Linear(64, 10),            \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "        \n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T04:10:32.309926200Z",
     "start_time": "2024-02-29T04:10:32.148913600Z"
    }
   },
   "id": "2c49abcd0c55eee7",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        X,y = X.to(device), y.to(device)\n",
    "        \n",
    "        pred = model(X)\n",
    "        loss= loss_fn(pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f'Loss: {loss:7f}[{current:>5d}/{size:>5d}]')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T04:10:32.820194100Z",
     "start_time": "2024-02-29T04:10:32.789200500Z"
    }
   },
   "id": "7dcca144cc2eed1c",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    # 꼭 넣어야한다.\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"test Error : \\n Accruacy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T04:10:33.503755400Z",
     "start_time": "2024-02-29T04:10:33.483756200Z"
    }
   },
   "id": "89c114d204780300",
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# SGD\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T04:36:30.305165800Z",
     "start_time": "2024-02-29T04:36:30.097168500Z"
    }
   },
   "id": "c22718cdb3183ff0",
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \n",
      "=============================\n",
      "Loss: 2.317459[    0/50000]\n",
      "Loss: 1.739805[ 6400/50000]\n",
      "Loss: 1.740267[12800/50000]\n",
      "Loss: 1.782589[19200/50000]\n",
      "Loss: 1.881373[25600/50000]\n",
      "Loss: 1.721865[32000/50000]\n",
      "Loss: 2.040204[38400/50000]\n",
      "Loss: 1.642022[44800/50000]\n",
      "test Error : \n",
      " Accruacy: 41.8%, Avg loss: 1.631086\n",
      "\n",
      "Epoch 2 \n",
      "=============================\n",
      "Loss: 1.530140[    0/50000]\n",
      "Loss: 1.631446[ 6400/50000]\n",
      "Loss: 1.697253[12800/50000]\n",
      "Loss: 1.503886[19200/50000]\n",
      "Loss: 1.699495[25600/50000]\n",
      "Loss: 1.374644[32000/50000]\n",
      "Loss: 1.568611[38400/50000]\n",
      "Loss: 1.776574[44800/50000]\n",
      "test Error : \n",
      " Accruacy: 45.1%, Avg loss: 1.537055\n",
      "\n",
      "Epoch 3 \n",
      "=============================\n",
      "Loss: 1.633435[    0/50000]\n",
      "Loss: 1.522249[ 6400/50000]\n",
      "Loss: 1.671011[12800/50000]\n",
      "Loss: 1.558997[19200/50000]\n",
      "Loss: 1.529529[25600/50000]\n",
      "Loss: 1.607464[32000/50000]\n",
      "Loss: 1.479077[38400/50000]\n",
      "Loss: 1.474301[44800/50000]\n",
      "test Error : \n",
      " Accruacy: 46.6%, Avg loss: 1.495029\n",
      "\n",
      "Epoch 4 \n",
      "=============================\n",
      "Loss: 1.441905[    0/50000]\n",
      "Loss: 1.620980[ 6400/50000]\n",
      "Loss: 1.420732[12800/50000]\n",
      "Loss: 1.257700[19200/50000]\n",
      "Loss: 1.401812[25600/50000]\n",
      "Loss: 1.490900[32000/50000]\n",
      "Loss: 1.472511[38400/50000]\n",
      "Loss: 1.426962[44800/50000]\n",
      "test Error : \n",
      " Accruacy: 47.7%, Avg loss: 1.493921\n",
      "\n",
      "Epoch 5 \n",
      "=============================\n",
      "Loss: 1.611702[    0/50000]\n",
      "Loss: 1.259854[ 6400/50000]\n",
      "Loss: 1.881349[12800/50000]\n",
      "Loss: 1.417064[19200/50000]\n",
      "Loss: 1.329258[25600/50000]\n",
      "Loss: 1.782189[32000/50000]\n",
      "Loss: 1.286790[38400/50000]\n",
      "Loss: 1.158620[44800/50000]\n",
      "test Error : \n",
      " Accruacy: 46.7%, Avg loss: 1.513295\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs =5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1} \\n=============================\")\n",
    "    train(trainloader, model, loss_fn, optimizer)\n",
    "    test(testloader, model,loss_fn)\n",
    "    optimizer.step()\n",
    "    \n",
    "print(\"Done!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T04:39:12.504573500Z",
     "start_time": "2024-02-29T04:36:31.429211100Z"
    }
   },
   "id": "8a9eaa4f0de2daea",
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3fd62f2addeda5a5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
